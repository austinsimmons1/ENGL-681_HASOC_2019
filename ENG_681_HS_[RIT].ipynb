{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ENG 681 - HS [RIT].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJPhpVNfclRu"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from matplotlib import pyplot as plt \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "from string import punctuation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import spacy\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG_O10Fipjbp"
      },
      "source": [
        "#SET random seed\n",
        "np.random.seed(1997)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9GOdvWYpl02"
      },
      "source": [
        "#Read the training & Test dataset using Pandas\n",
        "\n",
        "training = pd.read_csv(\"hasoc_train.csv\")\n",
        "test = pd.read_csv(\"hasoc_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "JWu02EDG46bL",
        "outputId": "ef1571f8-9210-421f-a69b-9a138f90bb41"
      },
      "source": [
        "#Recode labels for Training task_1 to numeric\n",
        "cleanup_nums = {\"task_1\": {\"HOF\": 1, \"NOT\": 0}}\n",
        "training = training.replace(cleanup_nums)\n",
        "training.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "      <th>task_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hasoc_en_1</td>\n",
              "      <td>#DhoniKeepsTheGlove | WATCH: Sports Minister K...</td>\n",
              "      <td>0</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hasoc_en_2</td>\n",
              "      <td>@politico No. We should remember very clearly ...</td>\n",
              "      <td>1</td>\n",
              "      <td>HATE</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hasoc_en_3</td>\n",
              "      <td>@cricketworldcup Guess who would be the winner...</td>\n",
              "      <td>0</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hasoc_en_4</td>\n",
              "      <td>Corbyn is too politically intellectual for #Bo...</td>\n",
              "      <td>0</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hasoc_en_5</td>\n",
              "      <td>All the best to #TeamIndia for another swimmin...</td>\n",
              "      <td>0</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      text_id                                               text  ...  task_2 task_3\n",
              "0  hasoc_en_1  #DhoniKeepsTheGlove | WATCH: Sports Minister K...  ...    NONE   NONE\n",
              "1  hasoc_en_2  @politico No. We should remember very clearly ...  ...    HATE    TIN\n",
              "2  hasoc_en_3  @cricketworldcup Guess who would be the winner...  ...    NONE   NONE\n",
              "3  hasoc_en_4  Corbyn is too politically intellectual for #Bo...  ...    NONE   NONE\n",
              "4  hasoc_en_5  All the best to #TeamIndia for another swimmin...  ...    NONE   NONE\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "Ip1QwWEN5M_l",
        "outputId": "715c4546-cee8-4f86-9d8c-28008b4248c9"
      },
      "source": [
        "#Recode labels for Test SubtaskA to numeric\n",
        "cleanup_nums2 = {\"task_1\": {\"HOF\": 1, \"NOT\": 0}}\n",
        "test = test.replace(cleanup_nums2)\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "      <th>task_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hasoc_en_902</td>\n",
              "      <td>West Bengal Doctor Crisis: Protesting doctors ...</td>\n",
              "      <td>0</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hasoc_en_416</td>\n",
              "      <td>68.5 million people have been forced to leave ...</td>\n",
              "      <td>0</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hasoc_en_207</td>\n",
              "      <td>You came, you saw .... we will look after the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hasoc_en_595</td>\n",
              "      <td>We'll get Brexit delivered by October 31st.   ...</td>\n",
              "      <td>0</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hasoc_en_568</td>\n",
              "      <td>Fuck you. Go back to the dark ages you cow @IB...</td>\n",
              "      <td>1</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>UNT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        text_id  ... task_3\n",
              "0  hasoc_en_902  ...   NONE\n",
              "1  hasoc_en_416  ...   NONE\n",
              "2  hasoc_en_207  ...   NONE\n",
              "3  hasoc_en_595  ...   NONE\n",
              "4  hasoc_en_568  ...    UNT\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB7qJJgXvuHz",
        "outputId": "ad9e9937-6d8a-4e48-fb9e-fa1a9c184d09"
      },
      "source": [
        "#Preview some Test tweets before processing\n",
        "a = 0 \n",
        "for i in range(a,a+10):\n",
        "    print(test[\"text\"][i])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "West Bengal Doctor Crisis: Protesting doctors agree to meet Mamata Banerjee in presence of full media even as IMA goes on strike  \n",
            "\n",
            "68.5 million people have been forced to leave their homes.      Read more: https://wef.ch/2YQcwpk  #refugees #society\n",
            "\n",
            "You came, you saw .... we will look after the fort! Good luck! \n",
            "\n",
            "We'll get Brexit delivered by October 31st.    Help build the movement that will do it  http://conservatives.com/join     #BackBoris @BorisJohnson\n",
            "\n",
            "Fuck you. Go back to the dark ages you cow @IBNLiveRealtime: Rapes happen because men and women interact freely: Mamata\n",
            "\n",
            "Boris Johnson faces Supreme Court bid to make him stand trial for ‘lying and misleading’ in Brexit campaign https://www.independent.co.uk/news/uk/crime/boris-johnson-brexit-supreme-court-vote-leave-nhs-bus-trial-appeal-a9029506.html …\n",
            "\n",
            "What about a refund for not serving Halala to Muslims and regularly adding Onions to Jain food !! Does Food not carries a religion here ??\n",
            "\n",
            "General election, DUP dumped out, Tory power weakened. The only way out\n",
            "\n",
            "#Repost free.wicked  • • • • • •  #freewicked #freethekids #terrorist #Americanterrorist #fucktrump #donaldtrump #fuckdonaldtrump #notothewall #nowall #ak47 #shooting #justiceforthekids #nomoregunviolence #gunviolence… https://www.instagram.com/p/B0lX5FzBnU7/?igshid=1joy9r96lwwg7 …\n",
            "\n",
            "Jesus Christ Christian News. Illuminati is now changing Bible into gay comedy book: God having bestiality sex, Peter running around naked, King David bowing Muslim way to gay lover Johnathan, 'wink mark' after incest verse, Jesus wearing girdle, men's milk https://www.youtube.com/watch?v=GhUsioGcknM … pic.twitter.com/SNcTq321bj\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdA56CcTs9Jg"
      },
      "source": [
        "def words(text, stem_words=False):\n",
        "    #Remove emojis \n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "    u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U000024C2-\\U0001F251\"\n",
        "    u\"\\U0001f926-\\U0001f937\"\n",
        "    u\"\\U00010000-\\U0010ffff\"\n",
        "    u\"\\u2640-\\u2642\"\n",
        "    u\"\\u2600-\\u2B55\"\n",
        "    u\"\\u200d\"\n",
        "    u\"\\u23cf\"\n",
        "    u\"\\u23e9\"\n",
        "    u\"\\u231a\"\n",
        "    u\"\\ufe0f\"  # dingbats\n",
        "    u\"\\u3030\"\n",
        "    \"]+\", flags=re.UNICODE)\n",
        "    \n",
        "    text = emoji_pattern.sub(r'', text) # No emoji\n",
        "    #text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE) # Remove urls\n",
        "    text = re.sub(r'http\\S+', '', text) # Remove urls\n",
        "    # Lowering the words in Tweets\n",
        "    text = text.lower()\n",
        "    # Remove punctuation from Tweets\n",
        "    text = ''.join([c for c in text if c not in punctuation])\n",
        "    #Use filter and lambda to remove numeric digits from string\n",
        "    res = \"\".join(filter(lambda x: not x.isdigit(), text))\n",
        "    text = str(res)\n",
        "    #remove leading and ending spaces\n",
        "    text = text.strip()\n",
        "    # Return a list of words\n",
        "    return(text)\n",
        "\n",
        "def process(tweet_list, tweets):\n",
        "    for tweet in tweets:\n",
        "        tweet_list.append(words(tweet))\n",
        "\n",
        "def tokenize(tweet_list, tweets):\n",
        "    for tweet in tweets:\n",
        "        tokens = re.findall(\"[\\w]+\", tweet)\n",
        "        tweet_list.append(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7OPTjnas0u3",
        "outputId": "f8636d18-c225-4742-8c86-2b59764bf275"
      },
      "source": [
        "#Processing training and test set\n",
        "processed_tweet1 = []\n",
        "processed_tweet2 = []\n",
        "process(processed_tweet1, training[\"text\"])\n",
        "process(processed_tweet2, test[\"text\"])\n",
        "\n",
        "#Preview some test tweets after processing\n",
        "a = 0 \n",
        "for i in range(a,a+10):\n",
        "    print(processed_tweet2[i])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "west bengal doctor crisis protesting doctors agree to meet mamata banerjee in presence of full media even as ima goes on strike\n",
            "\n",
            "million people have been forced to leave their homes      read more   refugees society\n",
            "\n",
            "you came you saw  we will look after the fort good luck\n",
            "\n",
            "well get brexit delivered by october st    help build the movement that will do it       backboris borisjohnson\n",
            "\n",
            "fuck you go back to the dark ages you cow ibnliverealtime rapes happen because men and women interact freely mamata\n",
            "\n",
            "boris johnson faces supreme court bid to make him stand trial for ‘lying and misleading’ in brexit campaign  …\n",
            "\n",
            "what about a refund for not serving halala to muslims and regularly adding onions to jain food  does food not carries a religion here\n",
            "\n",
            "general election dup dumped out tory power weakened the only way out\n",
            "\n",
            "repost freewicked  • • • • • •  freewicked freethekids terrorist americanterrorist fucktrump donaldtrump fuckdonaldtrump notothewall nowall ak shooting justiceforthekids nomoregunviolence gunviolence…  …\n",
            "\n",
            "jesus christ christian news illuminati is now changing bible into gay comedy book god having bestiality sex peter running around naked king david bowing muslim way to gay lover johnathan wink mark after incest verse jesus wearing girdle mens milk  … pictwittercomsnctqbj\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGujMqx1uyRF",
        "outputId": "88159de3-0bc7-48e8-d0af-46b83c40bc3b"
      },
      "source": [
        "tokenized_tweet1 = []\n",
        "tokenized_tweet2 = []\n",
        "tokenize(tokenized_tweet1, processed_tweet1)\n",
        "tokenize(tokenized_tweet2, processed_tweet2)\n",
        "\n",
        "#Preview some test tweets after tokenization\n",
        "a = 0 \n",
        "for i in range(a,a+10):\n",
        "    print(tokenized_tweet2[i])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['west', 'bengal', 'doctor', 'crisis', 'protesting', 'doctors', 'agree', 'to', 'meet', 'mamata', 'banerjee', 'in', 'presence', 'of', 'full', 'media', 'even', 'as', 'ima', 'goes', 'on', 'strike']\n",
            "\n",
            "['million', 'people', 'have', 'been', 'forced', 'to', 'leave', 'their', 'homes', 'read', 'more', 'refugees', 'society']\n",
            "\n",
            "['you', 'came', 'you', 'saw', 'we', 'will', 'look', 'after', 'the', 'fort', 'good', 'luck']\n",
            "\n",
            "['well', 'get', 'brexit', 'delivered', 'by', 'october', 'st', 'help', 'build', 'the', 'movement', 'that', 'will', 'do', 'it', 'backboris', 'borisjohnson']\n",
            "\n",
            "['fuck', 'you', 'go', 'back', 'to', 'the', 'dark', 'ages', 'you', 'cow', 'ibnliverealtime', 'rapes', 'happen', 'because', 'men', 'and', 'women', 'interact', 'freely', 'mamata']\n",
            "\n",
            "['boris', 'johnson', 'faces', 'supreme', 'court', 'bid', 'to', 'make', 'him', 'stand', 'trial', 'for', 'lying', 'and', 'misleading', 'in', 'brexit', 'campaign']\n",
            "\n",
            "['what', 'about', 'a', 'refund', 'for', 'not', 'serving', 'halala', 'to', 'muslims', 'and', 'regularly', 'adding', 'onions', 'to', 'jain', 'food', 'does', 'food', 'not', 'carries', 'a', 'religion', 'here']\n",
            "\n",
            "['general', 'election', 'dup', 'dumped', 'out', 'tory', 'power', 'weakened', 'the', 'only', 'way', 'out']\n",
            "\n",
            "['repost', 'freewicked', 'freewicked', 'freethekids', 'terrorist', 'americanterrorist', 'fucktrump', 'donaldtrump', 'fuckdonaldtrump', 'notothewall', 'nowall', 'ak', 'shooting', 'justiceforthekids', 'nomoregunviolence', 'gunviolence']\n",
            "\n",
            "['jesus', 'christ', 'christian', 'news', 'illuminati', 'is', 'now', 'changing', 'bible', 'into', 'gay', 'comedy', 'book', 'god', 'having', 'bestiality', 'sex', 'peter', 'running', 'around', 'naked', 'king', 'david', 'bowing', 'muslim', 'way', 'to', 'gay', 'lover', 'johnathan', 'wink', 'mark', 'after', 'incest', 'verse', 'jesus', 'wearing', 'girdle', 'mens', 'milk', 'pictwittercomsnctqbj']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qTKnEvOzG0W"
      },
      "source": [
        "# TFIDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5BXuWKByNEM"
      },
      "source": [
        "def dummy_fun(tokenized_tweet1):\n",
        "    return tokenized_tweet1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuO0Fezrts1g"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(analyzer = 'word',\n",
        "                        tokenizer=dummy_fun,\n",
        "                        preprocessor=dummy_fun,\n",
        "                        lowercase = True,\n",
        "                        max_features = 300,\n",
        "                        strip_accents='unicode',\n",
        "                        norm = 'l2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64_JIxlfzTPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010e24b2-7ad3-4149-896d-0bb653c51945"
      },
      "source": [
        "#training set\n",
        "tfidf1=tfidf.fit(tokenized_tweet1)\n",
        "transformed_tweets = tfidf1.transform(tokenized_tweet1)\n",
        "\n",
        "#test set \n",
        "transformed_tweets2 = tfidf.transform(tokenized_tweet2)\n",
        "\n",
        "#Assigning independent and dependent variables for training set\n",
        "x = transformed_tweets\n",
        "y = training['task_1']\n",
        "\n",
        "#Assigning independent and dependent variables for test set\n",
        "x1 = transformed_tweets2\n",
        "y1 = test['task_1']\n",
        "\n",
        "#Organizing variables into train and test set\n",
        "x_train = x\n",
        "y_train = y\n",
        "x_test = x1\n",
        "y_test = y1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoVYCgHmzj_R"
      },
      "source": [
        "# SVM (Linear SVM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9e1FqJMznPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9877d8ac-4914-4ff5-99af-2850a473fb09"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm = LinearSVC(tol=1e-05)\n",
        "svm.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=1e-05,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvVQVp9BztiO"
      },
      "source": [
        "#Predicting the test results\n",
        "y_pred = svm.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg97CB8h0g9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c818cf59-f758-46d8-8408-725a303efc7b"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred,labels=[1,0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.41      0.43      0.42       288\n",
            "           0       0.81      0.79      0.80       865\n",
            "\n",
            "    accuracy                           0.70      1153\n",
            "   macro avg       0.61      0.61      0.61      1153\n",
            "weighted avg       0.71      0.70      0.71      1153\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX55VWkH7l6-"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCA94Rth7rBC"
      },
      "source": [
        "from pandas import DataFrame\n",
        "#training set\n",
        "df0 = DataFrame(processed_tweet1,columns=['text'])\n",
        "#test set\n",
        "df00 = DataFrame(processed_tweet2,columns=['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NeQxatg7x3f"
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Sequential\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        " \n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "    return max([len(s.split()) for s in lines])\n",
        " \n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "    # integer encode\n",
        "    encoded = tokenizer.texts_to_sequences(lines)\n",
        "    # pad encoded sequences\n",
        "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "    return padded\n",
        " \n",
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "    # channel 1\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 100,input_shape=(length,)))\n",
        "    model.add(Conv1D(filters=32, kernel_size=4, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    # interpretation\n",
        "    model.add(Dense(1, activation='sigmoid')) \n",
        "    # compile\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # summarize\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-gsThWk7z8F",
        "outputId": "392f67e6-d39d-4c66-aecc-d1eafa150c9b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Organize  dataset\n",
        "x_train = df0['text']\n",
        "y_train = training['task_1']\n",
        "x_test = df00['text']\n",
        "y_test = test['task_1']\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(x_train)\n",
        "# calculate max document length\n",
        "length = max_length(x_train)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Max document length: %d' % length)\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, x_train, length)\n",
        "testX = encode_text(tokenizer, x_test, length)\n",
        "\n",
        "print(trainX.shape,testX.shape)\n",
        " \n",
        "# define model\n",
        "model = define_model(length, vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 92\n",
            "Vocabulary size: 17949\n",
            "(5852, 92) (1153, 92)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 92, 100)           1794900   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 89, 32)            12832     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 89, 32)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 44, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1408)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 1409      \n",
            "=================================================================\n",
            "Total params: 1,809,141\n",
            "Trainable params: 1,809,141\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wnsi-iHu7_la",
        "outputId": "f1a28733-7a12-4e98-b69e-d82c2694fcba"
      },
      "source": [
        "# fit model\n",
        "results = model.fit(trainX, array(y_train), epochs=10, batch_size=32, verbose=1,validation_data=(testX, array(y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "183/183 [==============================] - 23s 37ms/step - loss: 0.6687 - accuracy: 0.6145 - val_loss: 0.5825 - val_accuracy: 0.7502\n",
            "Epoch 2/10\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.5849 - accuracy: 0.6781 - val_loss: 0.5533 - val_accuracy: 0.7493\n",
            "Epoch 3/10\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.3032 - accuracy: 0.8985 - val_loss: 0.5933 - val_accuracy: 0.7112\n",
            "Epoch 4/10\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1195 - accuracy: 0.9671 - val_loss: 0.6377 - val_accuracy: 0.6869\n",
            "Epoch 5/10\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0666 - accuracy: 0.9819 - val_loss: 0.6821 - val_accuracy: 0.6973\n",
            "Epoch 6/10\n",
            "183/183 [==============================] - 6s 33ms/step - loss: 0.0527 - accuracy: 0.9836 - val_loss: 0.6885 - val_accuracy: 0.6973\n",
            "Epoch 7/10\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0379 - accuracy: 0.9874 - val_loss: 0.7395 - val_accuracy: 0.6878\n",
            "Epoch 8/10\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0352 - accuracy: 0.9866 - val_loss: 0.7396 - val_accuracy: 0.6982\n",
            "Epoch 9/10\n",
            "183/183 [==============================] - 6s 33ms/step - loss: 0.0314 - accuracy: 0.9887 - val_loss: 0.7618 - val_accuracy: 0.6904\n",
            "Epoch 10/10\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0249 - accuracy: 0.9931 - val_loss: 0.8053 - val_accuracy: 0.6791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UThaz0gG8IsN",
        "outputId": "72816301-6be2-48e0-c8a2-91a519f5412f"
      },
      "source": [
        "pred = model.predict_classes(testX)\n",
        "print(classification_report(y_test,pred,labels=[1,0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.41      0.62      0.49       288\n",
            "           0       0.85      0.70      0.77       865\n",
            "\n",
            "    accuracy                           0.68      1153\n",
            "   macro avg       0.63      0.66      0.63      1153\n",
            "weighted avg       0.74      0.68      0.70      1153\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}