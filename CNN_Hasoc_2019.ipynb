{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN Hasoc 2019.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybxPt2fL9gkV",
        "outputId": "a64fa551-3c1d-49d8-c760-81c6337e181e"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot as plt \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "from string import punctuation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import spacy\n",
        "import re\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class Model():\n",
        "\n",
        "    def __init__(self,datafile=\"hasoc\"):\n",
        "      self.training = pd.read_csv(datafile+ \"_train.csv\")\n",
        "      self.test = pd.read_csv(datafile+ \"_test.csv\")  \n",
        "      #Recode labels for Training task_1 to numeric\n",
        "      cleanup_nums = {\"task_1\": {\"HOF\": 1, \"NOT\": 0}}\n",
        "      self.training = self.training.replace(cleanup_nums)\n",
        "      #Recode labels for Test SubtaskA to numeric\n",
        "      cleanup_nums2 = {\"task_1\": {\"HOF\": 1, \"NOT\": 0}}\n",
        "      self.test = self.test.replace(cleanup_nums2)   \n",
        "    \n",
        "    def preprocess(self):\n",
        "      def words(text, stem_words=False):\n",
        "        #Remove emojis \n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "        text = emoji_pattern.sub(r'', text) # No emoji\n",
        "        #text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE) # Remove urls\n",
        "        text = re.sub(r'http\\S+', '', text) # Remove urls\n",
        "        # Lowering the words in Tweets\n",
        "        text = text.lower()\n",
        "        # Remove punctuation from Tweets\n",
        "        text = ''.join([c for c in text if c not in punctuation])\n",
        "        #Use filter and lambda to remove numeric digits from string\n",
        "        res = \"\".join(filter(lambda x: not x.isdigit(), text))\n",
        "        text = str(res)\n",
        "        #remove leading and ending spaces\n",
        "        text = text.strip()\n",
        "        # Return a list of words\n",
        "        return(text)\n",
        "\n",
        "      def process(tweet_list, tweets):\n",
        "          for tweet in tweets:\n",
        "              tweet_list.append(words(tweet))\n",
        "\n",
        "      self.processed_tweet1 = []\n",
        "      self.processed_tweet2 = []\n",
        "      process(self.processed_tweet1, self.training[\"text\"])\n",
        "      process(self.processed_tweet2, self.test[\"text\"])\n",
        "\n",
        "    def fit(self,):\n",
        "      from pandas import DataFrame\n",
        "      #training set\n",
        "      self.df0 = DataFrame(self.processed_tweet1,columns=['text'])\n",
        "      #test set\n",
        "      self.df00 = DataFrame(self.processed_tweet2,columns=['text'])   \n",
        "      # fit a tokenizer\n",
        "      def create_tokenizer(lines):\n",
        "          tokenizer = Tokenizer()\n",
        "          tokenizer.fit_on_texts(lines)\n",
        "          return tokenizer\n",
        "      # calculate the maximum document length\n",
        "      def max_length(lines):\n",
        "          return max([len(s.split()) for s in lines])\n",
        "      # encode a list of lines\n",
        "      def encode_text(tokenizer, lines, length):\n",
        "          # integer encode\n",
        "          encoded = tokenizer.texts_to_sequences(lines)\n",
        "          # pad encoded sequences\n",
        "          padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "          return padded\n",
        "      # define the model\n",
        "      def define_model(length, vocab_size):\n",
        "          # channel 1\n",
        "          model = Sequential()\n",
        "          model.add(Embedding(vocab_size, 100,input_shape=(length,)))\n",
        "          model.add(Conv1D(filters=32, kernel_size=4, activation='relu'))\n",
        "          model.add(Dropout(0.5))\n",
        "          model.add(MaxPooling1D(pool_size=2))\n",
        "          model.add(Flatten())\n",
        "          # interpretation\n",
        "          model.add(Dense(1, activation='sigmoid')) \n",
        "          # compile\n",
        "          model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "          # summarize\n",
        "          return model\n",
        "      # Organize  dataset\n",
        "      self.x_train = self.df0['text']\n",
        "      self.y_train = self.training['task_1']\n",
        "      self.x_test = self.df00['text']\n",
        "      self.y_test = self.test['task_1']\n",
        "      # create tokenizer\n",
        "      tokenizer = create_tokenizer(self.x_train)\n",
        "      # calculate max document length\n",
        "      length = max_length(self.x_train)\n",
        "      # calculate vocabulary size\n",
        "      vocab_size = len(tokenizer.word_index) + 1\n",
        "      # encode data\n",
        "      self.trainX = encode_text(tokenizer, self.x_train, length)\n",
        "      self.testX = encode_text(tokenizer, self.x_test, length)\n",
        "      self.model = define_model(length, vocab_size)\n",
        "      # fit model\n",
        "      results = self.model.fit(self.trainX, array(self.y_train), epochs=10, batch_size=32, verbose=1,validation_data=(self.testX, array(self.y_test)))\n",
        "    \n",
        "    def predict(self):\n",
        "      pred = self.model.predict_classes(self.testX)\n",
        "      print(classification_report(self.y_test,pred,labels=[1,0]))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_instance = Model()\n",
        "    model_instance.preprocess()\n",
        "    model_instance.fit()\n",
        "    model_instance.predict()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "183/183 [==============================] - 8s 37ms/step - loss: 0.6690 - accuracy: 0.6037 - val_loss: 0.6239 - val_accuracy: 0.7476\n",
            "Epoch 2/10\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.5759 - accuracy: 0.6999 - val_loss: 0.5682 - val_accuracy: 0.7372\n",
            "Epoch 3/10\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.2814 - accuracy: 0.9029 - val_loss: 0.6407 - val_accuracy: 0.6644\n",
            "Epoch 4/10\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.1212 - accuracy: 0.9703 - val_loss: 0.6400 - val_accuracy: 0.6800\n",
            "Epoch 5/10\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.0645 - accuracy: 0.9849 - val_loss: 0.6429 - val_accuracy: 0.6956\n",
            "Epoch 6/10\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.0413 - accuracy: 0.9876 - val_loss: 0.7454 - val_accuracy: 0.6644\n",
            "Epoch 7/10\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.0319 - accuracy: 0.9901 - val_loss: 0.7172 - val_accuracy: 0.6826\n",
            "Epoch 8/10\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.0316 - accuracy: 0.9902 - val_loss: 0.7331 - val_accuracy: 0.6895\n",
            "Epoch 9/10\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.0277 - accuracy: 0.9918 - val_loss: 0.7658 - val_accuracy: 0.6791\n",
            "Epoch 10/10\n",
            "183/183 [==============================] - 6s 36ms/step - loss: 0.0222 - accuracy: 0.9921 - val_loss: 0.7819 - val_accuracy: 0.6869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.41      0.55      0.47       288\n",
            "           0       0.83      0.73      0.78       865\n",
            "\n",
            "    accuracy                           0.69      1153\n",
            "   macro avg       0.62      0.64      0.62      1153\n",
            "weighted avg       0.72      0.69      0.70      1153\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}