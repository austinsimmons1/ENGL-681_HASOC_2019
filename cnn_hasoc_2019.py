# -*- coding: utf-8 -*-
"""CNN Hasoc 2019.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Cd9aVbLyblmKgsaV3BvlvvI6JcZnqKx
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from pandas import DataFrame
from matplotlib import pyplot as plt 
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
import re
from string import punctuation
from sklearn.metrics import classification_report, confusion_matrix
import spacy
import re
from numpy import array
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.layers import Embedding
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers.merge import concatenate
from keras.models import Sequential
from sklearn.model_selection import train_test_split

class Model():

    def __init__(self,datafile="hasoc"):
      self.training = pd.read_csv(datafile+ "_train.csv")
      self.test = pd.read_csv(datafile+ "_test.csv")  
      #Recode labels for Training task_1 to numeric
      cleanup_nums = {"task_1": {"HOF": 1, "NOT": 0}}
      self.training = self.training.replace(cleanup_nums)
      #Recode labels for Test SubtaskA to numeric
      cleanup_nums2 = {"task_1": {"HOF": 1, "NOT": 0}}
      self.test = self.test.replace(cleanup_nums2)   
    
    def preprocess(self):
      def words(text, stem_words=False):
        #Remove emojis 
        emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002500-\U00002BEF"  # chinese char
        u"\U00002702-\U000027B0"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        u"\u2640-\u2642"
        u"\u2600-\u2B55"
        u"\u200d"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"  # dingbats
        u"\u3030"
        "]+", flags=re.UNICODE)
        text = emoji_pattern.sub(r'', text) # No emoji
        #text = re.sub(r'^https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE) # Remove urls
        text = re.sub(r'http\S+', '', text) # Remove urls
        # Lowering the words in Tweets
        text = text.lower()
        # Remove punctuation from Tweets
        text = ''.join([c for c in text if c not in punctuation])
        #Use filter and lambda to remove numeric digits from string
        res = "".join(filter(lambda x: not x.isdigit(), text))
        text = str(res)
        #remove leading and ending spaces
        text = text.strip()
        # Return a list of words
        return(text)

      def process(tweet_list, tweets):
          for tweet in tweets:
              tweet_list.append(words(tweet))

      self.processed_tweet1 = []
      self.processed_tweet2 = []
      process(self.processed_tweet1, self.training["text"])
      process(self.processed_tweet2, self.test["text"])

    def fit(self,):
      from pandas import DataFrame
      #training set
      self.df0 = DataFrame(self.processed_tweet1,columns=['text'])
      #test set
      self.df00 = DataFrame(self.processed_tweet2,columns=['text'])   
      # fit a tokenizer
      def create_tokenizer(lines):
          tokenizer = Tokenizer()
          tokenizer.fit_on_texts(lines)
          return tokenizer
      # calculate the maximum document length
      def max_length(lines):
          return max([len(s.split()) for s in lines])
      # encode a list of lines
      def encode_text(tokenizer, lines, length):
          # integer encode
          encoded = tokenizer.texts_to_sequences(lines)
          # pad encoded sequences
          padded = pad_sequences(encoded, maxlen=length, padding='post')
          return padded
      # define the model
      def define_model(length, vocab_size):
          # channel 1
          model = Sequential()
          model.add(Embedding(vocab_size, 100,input_shape=(length,)))
          model.add(Conv1D(filters=32, kernel_size=4, activation='relu'))
          model.add(Dropout(0.5))
          model.add(MaxPooling1D(pool_size=2))
          model.add(Flatten())
          # interpretation
          model.add(Dense(1, activation='sigmoid')) 
          # compile
          model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
          # summarize
          return model
      # Organize  dataset
      self.x_train = self.df0['text']
      self.y_train = self.training['task_1']
      self.x_test = self.df00['text']
      self.y_test = self.test['task_1']
      # create tokenizer
      tokenizer = create_tokenizer(self.x_train)
      # calculate max document length
      length = max_length(self.x_train)
      # calculate vocabulary size
      vocab_size = len(tokenizer.word_index) + 1
      # encode data
      self.trainX = encode_text(tokenizer, self.x_train, length)
      self.testX = encode_text(tokenizer, self.x_test, length)
      self.model = define_model(length, vocab_size)
      # fit model
      results = self.model.fit(self.trainX, array(self.y_train), epochs=10, batch_size=32, verbose=1,validation_data=(self.testX, array(self.y_test)))
    
    def predict(self):
      pred = self.model.predict_classes(self.testX)
      print(classification_report(self.y_test,pred,labels=[1,0]))

if __name__ == '__main__':
    model_instance = Model()
    model_instance.preprocess()
    model_instance.fit()
    model_instance.predict()